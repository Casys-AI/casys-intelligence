# AgentCards - Epic Breakdown

**Author:** BMad
**Date:** 2025-11-03 (Updated: 2025-11-24)
**Project Level:** 3
**Target Scale:** 8 epics, 37+ stories total (baseline + adaptive features)

---

## Overview

Ce document fournit le breakdown d√©taill√© des epics pour AgentCards, compl√©tant la [PRD](./PRD.md) strat√©gique.

Chaque epic inclut:
- Expanded goal et value proposition
- Complete story breakdown avec user stories
- Acceptance criteria pour chaque story
- Story sequencing et dependencies

**Epic Sequencing Principles:**
- Epic 1 √©tablit l'infrastructure fondamentale et context optimization
- Epic 2 ajoute la parall√©lisation et production readiness
- Stories dans epics sont vertically sliced et sequentially ordered
- No forward dependencies - chaque story build uniquement sur previous work

---

## Epic 1: Project Foundation & Context Optimization Engine

**Expanded Goal (2-3 sentences):**

√âtablir l'infrastructure projet Deno avec CI/CD, impl√©menter le syst√®me de vector search s√©mantique via PGlite + pgvector, et cr√©er le moteur de context optimization qui r√©duit la consommation de contexte de 30-50% √† <5%. Ce premier epic livre un syst√®me fonctionnel permettant le chargement on-demand des tool schemas MCP, validant la proposition de valeur principale d'AgentCards et √©tablissant les foundations pour la parall√©lisation (Epic 2).

**Value Delivery:**

√Ä la fin de cet epic, un d√©veloppeur peut installer AgentCards, migrer sa configuration MCP, et observer imm√©diatement une r√©duction du contexte √† <5%, r√©cup√©rant 90% de sa fen√™tre conversationnelle pour usage utile.

---

### Story Breakdown - Epic 1

**Story 1.1: Project Setup & Repository Structure**

As a developer,
I want a clean Deno project structure with CI/CD configured,
So that I can start development with proper tooling and automation in place.

**Acceptance Criteria:**
1. Repository initialis√© avec structure Deno standard (src/, tests/, docs/)
2. GitHub Actions CI configur√© (lint, typecheck, tests)
3. deno.json configur√© avec tasks scripts (test, lint, fmt, dev)
4. README.md avec badges CI et quick start guide
5. .gitignore appropri√© pour Deno projects
6. License MIT et CODE_OF_CONDUCT.md

**Prerequisites:** None

---

**Story 1.2: PGlite Database Foundation with pgvector**

As a developer,
I want a PGlite database with pgvector extension configured,
So that I can store embeddings vectoriels et perform semantic search efficiently.

**Acceptance Criteria:**
1. PGlite database initialization dans `~/.agentcards/.agentcards.db`
2. pgvector extension loaded et operational
3. Database schema cr√©√© avec tables:
   - `tool_embedding` (tool_id, embedding vector(1024), metadata)
   - `tool_schema` (tool_id, schema_json, server_id, cached_at)
   - `config` (key, value pour metadata)
4. Vector index HNSW cr√©√© sur tool_embedding.embedding avec pgvector
5. Basic CRUD operations test√©s (insert, query, update, delete)
6. Database migration system en place pour schema evolution future

**Prerequisites:** Story 1.1 (project setup)

---

**Story 1.3: MCP Server Discovery & Schema Extraction**

As a power user with 15+ MCP servers,
I want AgentCards to automatically discover my MCP servers and extract their tool schemas,
So that I don't have to manually configure each server.

**Acceptance Criteria:**
1. MCP server discovery via stdio et SSE protocols
2. Connection √©tablie avec chaque discovered server
3. Tool schemas extracted via MCP protocol `list_tools` call
4. Schemas parsed et validated (input/output schemas, descriptions)
5. Schemas stock√©s dans PGlite `tool_schema` table
6. Error handling pour servers unreachable ou invalid schemas
7. Console output affiche nombre de servers discovered et tools extracted
8. Support au minimum 15 MCP servers simultan√©ment

**Prerequisites:** Story 1.2 (database foundation)

---

**Story 1.4: Embeddings Generation with BGE-Large-EN-v1.5**

As a developer,
I want tool schemas to be converted into vector embeddings using BGE-Large-EN-v1.5 locally,
So that I can perform semantic search without relying on external APIs.

**Acceptance Criteria:**
1. BGE-Large-EN-v1.5 model downloaded et loaded (via @xenova/transformers)
2. Tool schemas (name + description + parameters) concaten√©s en text input
3. Embeddings (1024-dim) g√©n√©r√©s pour chaque tool
4. Embeddings stock√©s dans `tool_embeddings` table avec metadata
5. Progress bar affich√©e durant g√©n√©ration (peut prendre ~60s pour 100+ tools)
6. Embeddings cach√©s (pas de r√©g√©n√©ration si schema unchanged)
7. Total generation time <2 minutes pour 200 tools

**Prerequisites:** Story 1.3 (schema extraction)

---

**Story 1.5: Semantic Vector Search Implementation**

As a developer,
I want to search for relevant tools using natural language queries,
So that I can find the right tools without knowing their exact names.

**Acceptance Criteria:**
1. Query embedding g√©n√©ration (m√™me mod√®le BGE-Large-EN-v1.5)
2. Cosine similarity search sur vector index (<100ms query time P95)
3. API: `searchTools(query: string, topK: number)` ‚Üí tool_ids + scores
4. Top-k results returned sorted par relevance score (default k=5)
5. Configurable similarity threshold (default 0.7)
6. Unit tests validant accuracy avec sample queries
7. Benchmark test confirmant P95 <100ms pour 1000+ vectors

**Prerequisites:** Story 1.4 (embeddings generation)

---

**Story 1.6: On-Demand Schema Loading & Context Optimization**

As a Claude Code user,
I want AgentCards to load only relevant tool schemas based on my query,
So that my context window is not saturated by unused tool schemas.

**Acceptance Criteria:**
1. Integration semantic search avec schema loading
2. Workflow: query ‚Üí vector search ‚Üí retrieve top-k tools ‚Üí load schemas
3. Schemas retourn√©s uniquement pour matched tools (pas all-at-once)
4. Context usage measurement et logging (<5% target)
5. Comparison metric affich√©: before (30-50%) vs after (<5%)
6. Cache hit pour frequently used tools (√©vite reloading)
7. Performance: Total query-to-schema latency <200ms P95

**Prerequisites:** Story 1.5 (vector search)

---

**Story 1.7: Migration Tool (`agentcards init`)**

As a power user with existing MCP configuration,
I want to migrate my mcp.json configuration to AgentCards automatically,
So that I don't have to manually reconfigure everything.

**Acceptance Criteria:**
1. CLI command `agentcards init` implemented
2. Detection automatique du claude_desktop_config.json path (OS-specific)
3. Parsing du mcp.json existant et extraction des MCP servers
4. Generation de `~/.agentcards/config.yaml` avec servers migr√©s
5. Embeddings generation triggered automatiquement post-migration
6. Console output avec instructions pour √©diter mcp.json
7. Template affich√© pour nouvelle config mcp.json (juste agentcards gateway)
8. Rollback capability si erreur durant migration
9. Dry-run mode (`--dry-run`) pour preview changes

**Prerequisites:** Story 1.6 (context optimization functional)

---

**Story 1.8: Basic Logging & Telemetry Backend**

As a developer,
I want structured logging et m√©triques telemetry opt-in,
So that I can debug issues et measure success metrics (context usage, latency).

**Acceptance Criteria:**
1. Structured logging avec std/log (Deno standard library)
2. Log levels: error, warn, info, debug
3. Log output: console + file (`~/.agentcards/logs/agentcards.log`)
4. Telemetry table dans PGlite: `metrics` (timestamp, metric_name, value)
5. Metrics tracked: context_usage_pct, query_latency_ms, tools_loaded_count
6. Opt-in consent prompt au premier launch (telemetry disabled by default)
7. CLI flag `--telemetry` pour enable/disable
8. Privacy: aucune data sensitive (queries, schemas) ne quitte local machine

**Prerequisites:** Story 1.7 (migration tool ready)

---

## Epic 2: DAG Execution & Production Readiness

**Expanded Goal (2-3 sentences):**

Impl√©menter le syst√®me de DAG execution pour parall√©lisation intelligente des workflows multi-tools, int√©grer AgentCards comme MCP gateway avec Claude Code, et hardening production avec health checks, error handling robuste, et tests end-to-end. Ce second epic livre un syst√®me production-ready capable de r√©duire la latence des workflows de 5x √† 1x via parall√©lisation, compl√©tant ainsi la double value proposition d'AgentCards (context + speed).

**Architecture Clarification: GraphRAG vs DAG:**

Il est crucial de comprendre la distinction entre deux composants architecturaux compl√©mentaires :

- **GraphRAG (Epic 1)** = Base de connaissances globale
  - Stocke TOUS les tools de TOUS les MCP servers (687 tools)
  - Contient l'historique des workflows ex√©cut√©s (succ√®s/√©checs, patterns)
  - Maintient les relations entre tools (ex: "filesystem:read" suivi de "json:parse" dans 85% des cas)
  - **Scope:** Global, toutes les possibilit√©s

- **DAG (Epic 2)** = Instance de workflow sp√©cifique
  - Un workflow concret pour UNE t√¢che pr√©cise
  - Contient uniquement les 3-5 tools pertinents pour cette requ√™te
  - D√©finit explicitement les d√©pendances (task B d√©pend de task A)
  - **Scope:** Local, single execution

**Comment ils permettent le Speculative Execution:**

```
GraphRAG (Epic 1) ‚Üí Apprend les patterns historiques
        ‚Üì
DAG Suggester ‚Üí Pr√©dit quel DAG construire bas√© sur l'intent
        ‚Üì
DAG (Epic 2) ‚Üí Structure concr√®te √† ex√©cuter
        ‚Üì
Execution Sp√©culative ‚Üí Lance le DAG pr√©dit AVANT que l'agent demande
        ‚Üì
R√©sultats cach√©s ‚Üí Agent obtient r√©ponse instantan√©e
```

Sans GraphRAG (la connaissance), impossible de pr√©dire quel DAG construire.
Sans DAG (la structure), impossible d'ex√©cuter en parall√®le ou sp√©culativement.

Le **Speculative Execution** n'est possible que gr√¢ce au **graph de d√©pendances** qui encode les patterns appris dans GraphRAG et permet la pr√©diction de workflows complets.

**Value Delivery:**

√Ä la fin de cet epic, un d√©veloppeur peut ex√©cuter des workflows cross-MCP complexes avec parall√©lisation automatique, observant des gains de performance 3-5x sur workflows typiques, le tout via une gateway stable et fiable int√©gr√©e √† Claude Code.

---

### Story Breakdown - Epic 2

**Story 2.1: Dependency Graph Construction (DAG Builder)**

As a developer,
I want AgentCards to automatically construct a dependency graph from tool input/output schemas,
So that independent tools can be identified for parallel execution.

**Acceptance Criteria:**
1. DAG builder module cr√©√© (`src/dag/builder.ts`)
2. Parsing des tool input/output schemas (JSON Schema format)
3. Dependency detection: tool B depends on tool A si output_A matches input_B
4. DAG representation: nodes (tools) + edges (dependencies)
5. Topological sort implementation (custom, zero external dependency)
6. Detection de cycles (DAG invalide) avec error reporting
7. Unit tests avec sample workflows (sequential, parallel, mixed)
8. API: `buildDAG(tools: Tool[])` ‚Üí DAG graph object

**Prerequisites:** Epic 1 compl√©t√© (context optimization functional)

---

**Story 2.2: Parallel Execution Engine**

As a power user,
I want workflows avec independent tools to execute in parallel,
So that I save time instead of waiting for sequential execution.

**Acceptance Criteria:**
1. Parallel executor module cr√©√© (`src/dag/executor.ts`)
2. DAG traversal avec identification des nodes ex√©cutables en parall√®le
3. Promise.all utilis√© pour parallel execution de independent branches
4. Sequential execution pour dependent tools (respect topological order)
5. Partial success handling: continue execution m√™me si un tool fail
6. Results aggregation: successes + errors retourn√©s avec codes
7. Performance measurement: latency avant/apr√®s parall√©lisation
8. Target: P95 latency <3 secondes pour workflow 5-tools
9. Benchmarks tests validant 3-5x speedup sur workflows parall√©lisables

**Prerequisites:** Story 2.1 (DAG builder)

---

**Story 2.3: SSE Streaming pour Progressive Results**

As a user waiting for workflow results,
I want to see results streamed progressively as they complete,
So that I get feedback immediately instead of waiting for all tools to finish.

**Acceptance Criteria:**
1. SSE (Server-Sent Events) implementation pour streaming
2. Event types d√©finis: `task_start`, `task_complete`, `execution_complete`, `error`
3. Results stream√©s d√®s disponibilit√© (pas de wait-all-then-return)
4. Event payload: tool_id, status, result, timestamp
5. Client-side handling simul√© dans tests
6. Graceful degradation si SSE unavailable (fallback to batch response)
7. Max event buffer size pour √©viter memory leaks

**Prerequisites:** Story 2.2 (parallel executor)

---

**Story 2.4: MCP Gateway Integration avec Claude Code**

As a Claude Code user,
I want AgentCards to act as a transparent MCP gateway,
So that Claude can interact with all my MCP servers via a single entry point.

**Acceptance Criteria:**
1. MCP protocol server implementation (stdio mode primary)
2. AgentCards expose MCP server interface compatible avec Claude Code
3. Requests de Claude intercept√©s par gateway
4. Vector search ‚Üí load schemas ‚Üí execute tools ‚Üí return results
5. Transparent proxying: Claude voit AgentCards comme un seul MCP server
6. Support `list_tools`, `call_tool`, `get_prompt` methods (MCP spec)
7. Error handling: MCP-compliant error responses
8. Integration test avec mock Claude client

**Prerequisites:** Story 2.3 (SSE streaming ready)

---

**Story 2.5: Health Checks & MCP Server Monitoring**

As a developer,
I want AgentCards to monitor MCP server health et report issues,
So that I know which servers are down or misconfigured.

**Acceptance Criteria:**
1. Health check implementation au startup (ping chaque MCP server)
2. Periodic health checks (every 5 minutes) durant runtime
3. Health status tracking: healthy, degraded, down
4. Console warnings pour servers unavailable
5. Automatic retry logic (3 attempts) avant marking server down
6. Health status API: `agentcards status` CLI command
7. Logs structured avec server_id, status, last_check timestamp

**Prerequisites:** Story 2.4 (gateway integration)

---

**Story 2.6: Error Handling & Resilience**

As a developer,
I want robust error handling throughout AgentCards,
So that the system degrades gracefully instead of crashing.

**Acceptance Criteria:**
1. Try-catch wrappers autour de all async operations
2. Error types d√©finis: MCPServerError, VectorSearchError, DAGExecutionError
3. User-friendly error messages avec suggestions de resolution
4. Rollback capability pour failed migrations
5. Partial workflow success (return succ√®s m√™me si some tools fail)
6. Timeout handling (default 30s per tool execution)
7. Rate limiting pour prevent MCP server overload
8. Error logs persist√©s pour post-mortem analysis

**Prerequisites:** Story 2.5 (health checks)

---

**Story 2.7: End-to-End Tests & Production Hardening**

As a developer shipping production software,
I want comprehensive E2E tests et production hardening,
So that AgentCards is reliable et users don't experience bugs.

**Acceptance Criteria:**
1. E2E test suite cr√©√© avec Deno.test
2. Test scenarios: migration, vector search, DAG execution, gateway proxying
3. Mock MCP servers pour testing (fixtures)
4. Integration tests avec real BGE-Large model
5. Performance regression tests (benchmark suite)
6. Memory leak detection tests (long-running daemon)
7. CI configuration updated pour run E2E tests
8. Code coverage report >80% (unit + integration)
9. Load testing: 15+ MCP servers, 100+ tools
10. Documentation: README updated avec installation, usage, troubleshooting

**Prerequisites:** Story 2.6 (error handling)

---

## Epic 2.5: Adaptive DAG Feedback Loops (Foundation)

**Expanded Goal (2-3 sentences):**

√âtablir la fondation pour workflows adaptatifs avec feedback loops Agent-in-the-Loop (AIL) et Human-in-the-Loop (HIL), pr√©parant l'int√©gration avec Epic 3 (Sandbox). Impl√©menter l'architecture 3-Loop Learning (Phase 1 - Foundation) avec event stream observable, checkpoint/resume, et DAG replanning dynamique. Ce pivot architectural d√©bloque le contr√¥le runtime essentiel pour les op√©rations critiques (HIL approval code sandbox Epic 3) et workflows adaptatifs d√©couvrant progressivement leurs besoins.

**Architecture 3-Loop Learning (Phase 1 - Foundation):**

**Loop 1 (Execution - Real-time):**
- Event stream observable pour monitoring en temps r√©el
- Command queue pour contr√¥le dynamique (agent + humain)
- State management avec checkpoints et resume
- **Fr√©quence:** Milliseconds (pendant l'ex√©cution)

**Loop 2 (Adaptation - Runtime):**
- Agent-in-the-Loop (AIL): D√©cisions autonomes pendant l'ex√©cution
- Human-in-the-Loop (HIL): Validation humaine pour op√©rations critiques
- DAG re-planning dynamique via GraphRAG queries
- **Fr√©quence:** Seconds √† minutes (entre layers)

**Loop 3 (Meta-Learning - Basic):**
- GraphRAG updates from execution patterns (co-occurrence, preferences)
- Learning baseline pour futures optimisations
- **Fr√©quence:** Per-workflow

**Value Delivery:**

√Ä la fin de cet epic, AgentCards peut adapter ses workflows en temps r√©el bas√© sur les d√©couvertes runtime, demander validation humaine pour op√©rations critiques, et apprendre des patterns d'ex√©cution pour am√©liorer futures suggestions. Foundation critique pour Epic 3 (HIL code sandbox approval) et Epic 3.5 (speculation with rollback).

---

### Story Breakdown - Epic 2.5

**Story 2.5-1: Event Stream, Command Queue & State Management**

As a developer building adaptive workflows,
I want real-time event streaming and dynamic control capabilities,
So that I can observe execution progress and inject commands during runtime.

**Acceptance Criteria:**
1. `ControlledExecutor` extends `ParallelExecutor` (Epic 2) avec event stream
2. Event types d√©finis: `workflow_started`, `task_started`, `task_completed`, `workflow_completed`, `error`, `awaiting_input`
3. EventEmitter implementation (Node.js-style events)
4. Command queue: `pause`, `resume`, `cancel`, `replan`, `inject_task`
5. State management: workflow state = `{ status, current_tasks, completed_tasks, pending_tasks, checkpoints }`
6. State serialization/deserialization (JSON-compatible)
7. Thread-safe command injection (async queue)
8. Unit tests: event emission, command processing, state transitions
9. Integration test: Execute workflow ‚Üí inject pause command ‚Üí verify workflow pauses

**Prerequisites:** Epic 2 completed (ParallelExecutor functional)

---

**Story 2.5-2: Checkpoint & Resume Infrastructure**

As a user with long-running workflows,
I want workflows to be resumable after interruptions,
So that I don't lose progress if something fails or I need to stop.

**Acceptance Criteria:**
1. Checkpoint syst√®me impl√©ment√© (`src/dag/checkpoint.ts`)
2. Checkpoints stock√©s dans PGlite table: `workflow_checkpoints` (workflow_id, state_json, timestamp)
3. Checkpoint automatique: apr√®s chaque task completed, before each critical operation
4. Resume API: `resumeWorkflow(workflow_id)` ‚Üí reconstruit state et continue
5. Partial result preservation: completed tasks results cached
6. Task idempotency verification: detect if task already completed before retry
7. Checkpoint cleanup: auto-delete checkpoints >7 days old
8. CLI command: `agentcards resume <workflow_id>`
9. Error handling: corrupt checkpoint ‚Üí fallback to nearest valid checkpoint
10. Integration test: Workflow fails mid-execution ‚Üí resume ‚Üí completes successfully

**Prerequisites:** Story 2.5-1 (state management)

---

**Story 2.5-3: AIL/HIL Integration & DAG Replanning**

As an AI agent executing complex workflows,
I want to make autonomous decisions (AIL) and request human validation (HIL) when needed,
So that workflows can adapt based on discoveries and critical operations get human oversight.

**Acceptance Criteria:**
1. AIL (Agent-in-the-Loop) implementation:
   - Decision points d√©finis dans DAG: `{ type: 'ail_decision', prompt: string, options: [...] }`
   - Agent query mechanism via single conversation thread (no context filtering)
   - Multi-turn conversation support for complex decisions
   - Decision logging dans PGlite: `ail_decisions` (workflow_id, decision_point, chosen_option, rationale)

2. HIL (Human-in-the-Loop) implementation:
   - Approval gates pour critical operations: `{ type: 'hil_approval', operation: string, risk_level: 'low'|'medium'|'high' }`
   - User prompt via CLI or API: "Approve code execution? [y/n]"
   - Timeout handling: auto-reject after 5 minutes (configurable)
   - Approval history logging

3. DAG Replanning:
   - `DAGSuggester.replanDAG(current_state, new_intent)` method
   - GraphRAG query pour find alternative paths
   - Merge new DAG avec existing execution state
   - Preserve completed tasks, replace pending tasks
   - Validation: no cycles introduced, dependencies preserved

4. Integration with ControlledExecutor:
   - Pause workflow at decision/approval points
   - Emit `awaiting_input` event
   - Resume after decision/approval received

5. Tests:
   - AIL test: Workflow encounters decision point ‚Üí agent chooses option ‚Üí workflow continues
   - HIL test: Critical operation ‚Üí human approves ‚Üí execution proceeds
   - Replanning test: Workflow discovers new requirement ‚Üí replan ‚Üí new tasks added
   - Multi-turn test: Agent asks follow-up questions before decision

**Prerequisites:** Story 2.5-2 (checkpoint/resume)

---

**Story 2.5-4: Command Infrastructure Hardening** *(Scope Reduced per ADR-018)*

> **UPDATE 2025-11-24:** Original scope (8 command handlers, 16h) reduced to 4h per **ADR-018: Command Handlers Minimalism**. Focus on production-blocking bug fixes and error handling, not new handlers.

As a developer building adaptive workflows,
I want robust command infrastructure with proper error handling,
So that the existing 4 core commands operate reliably in production.

**Acceptance Criteria:**
1. Fix BUG-001: Race condition in CommandQueue.processCommands()
   - Async/await properly handles Promise resolution
   - No commands lost during parallel processing
   - Integration tests verify fix

2. Improve command registry error handling:
   - Centralized command dispatch with Map registry
   - Try/catch wrappers around all handlers
   - Error events emitted for observability
   - Unknown commands logged as warnings (not errors)

3. Document Replan-First Architecture (ADR-018):
   - Update story with ADR-018 rationale
   - Add note to spike (over-scoping correction)
   - Update engineering backlog with deferred handlers

**Deferred Handlers** (See ADR-018 + engineering-backlog.md):
- ‚ùå `inject_tasks` - Redundant with `replan_dag`
- ‚ùå `skip_layer` - Safe-to-fail branches cover this
- ‚ùå `modify_args` - No proven HIL correction workflow yet
- ‚ùå `checkpoint_response` - Composition of existing handlers sufficient

**Prerequisites:** Story 2.5-3 (AIL/HIL integration)

**Related:** Engineering Backlog (BUG-001: Race condition in processCommands() should be fixed as part of this story)

---

## Story Guidelines Reference

**Story Format:**

```
**Story [EPIC.N]: [Story Title]**

As a [user type],
I want [goal/desire],
So that [benefit/value].

**Acceptance Criteria:**
1. [Specific testable criterion]
2. [Another specific criterion]
3. [etc.]

**Prerequisites:** [Dependencies on previous stories, if any]
```

**Story Requirements:**

- **Vertical slices** - Complete, testable functionality delivery
- **Sequential ordering** - Logical progression within epic
- **No forward dependencies** - Only depend on previous work
- **AI-agent sized** - Completable in 2-4 hour focused session
- **Value-focused** - Integrate technical enablers into value-delivering stories

---

## Epic 3: Agent Code Execution & Local Processing

**Expanded Goal (2-3 sentences):**

Impl√©menter un environnement d'ex√©cution s√©curis√© permettant aux agents d'√©crire et d'ex√©cuter du code TypeScript localement, traitant les donn√©es volumineuses avant injection dans le contexte LLM. Ce troisi√®me epic ajoute une couche de processing local compl√©mentaire au vector search (Epic 1) et au DAG execution (Epic 2), permettant de r√©duire davantage la consommation de contexte (de <5% √† <1%) pour les cas d'usage avec large datasets, tout en prot√©geant les donn√©es sensibles via tokenisation automatique des PII.

**Value Delivery:**

√Ä la fin de cet epic, un d√©veloppeur peut ex√©cuter des workflows qui traitent localement des datasets volumineux (ex: 1000 commits GitHub), filtrent et agr√®gent les donn√©es dans un sandbox s√©curis√©, et retournent seulement le r√©sum√© pertinent (<1KB) au lieu des donn√©es brutes (>1MB), r√©cup√©rant 99%+ de contexte additionnel et prot√©geant automatiquement les donn√©es sensibles.

**Estimation:** 8 stories (3.1 √† 3.8)

**Design Philosophy:**

Inspir√© par l'approche Anthropic de code execution, Epic 3 combine le meilleur des deux mondes : vector search (Epic 1) pour d√©couvrir les tools pertinents, puis code execution pour traiter les r√©sultats localement. L'agent √©crit du code au lieu d'appeler directement les tools, permettant filtrage, agr√©gation, et transformation avant que les donn√©es n'atteignent le contexte LLM.

**Safe-to-Fail Branches Pattern (Story 3.5 - Synergie avec Speculative Execution d'Epic 2):**

Une propri√©t√© architecturale critique qui √©merge d√®s que le sandbox est int√©gr√© au DAG (Story 3.5, juste apr√®s `execute_code` tool en 3.4) est le pattern des **branches safe-to-fail** : les t√¢ches sandbox peuvent √©chouer sans compromettre l'ensemble du workflow, car elles s'ex√©cutent dans un environnement isol√©. Contrairement aux appels MCP directs (qui peuvent avoir des effets de bord - √©criture fichier, cr√©ation issue GitHub), le code sandbox est **idempotent et isol√©**.

**Cette propri√©t√© d√©bloque la vraie puissance du Speculative Execution (Epic 2)** : avec les MCP tools directs, l'ex√©cution sp√©culative est risqu√©e (pr√©diction incorrecte = side effect ind√©sirable), mais avec le sandbox, tu peux :

**S√©quence logique Epic 3** : Foundation (3.1) ‚Üí Tools Injection (3.2) ‚Üí Data Processing (3.3) ‚Üí execute_code Tool (3.4) ‚Üí **Safe-to-Fail Branches (3.5)** ‚Üí PII Protection (3.6) ‚Üí Caching (3.7) ‚Üí E2E Tests (3.8). Safe-to-fail arrive d√®s que execute_code est op√©rationnel, car c'est une propri√©t√© architecturale du syst√®me, pas une optimization tardive.

1. **Intelligent Environment Isolation** : Le gateway ex√©cute les op√©rations lourdes dans un environnement s√©par√© invisible √† l'agent. L'agent ne voit jamais les donn√©es brutes (1000 commits = 1.2MB), seulement le r√©sultat trait√© (summary = 2KB). Pas de pollution de contexte.

2. **Aggressive Speculation Without Risk** : Pr√©dire et ex√©cuter plusieurs approches simultan√©ment (fast/ML/stats) sans risque. Si les pr√©dictions sont incorrectes, on discard les r√©sultats (pas de side effects). Si correctes, l'agent obtient une analyse multi-perspective instantan√©e.

3. **Resilient Workflows** : Lancer 3 approches d'analyse en parall√®le, utiliser celle qui r√©ussit. Les MCP tools ne peuvent pas faire √ßa (side effects, retry = duplicates), mais les sandbox branches le peuvent (failures are free, successes are valuable).

4. **Graceful Degradation** : Si l'analyse ML timeout, fallback automatique sur l'analyse statistique. Pas de rollback n√©cessaire, les branches √©chou√©es sont juste ignor√©es.

5. **Retry Safety** : R√©ex√©cuter des branches sandbox sans risque de duplication d'effets (idempotent).

6. **A/B Testing en Production** : Tester 2 algorithmes en parall√®le, comparer les r√©sultats, choisir le meilleur.

Le combo **Speculative Execution (Epic 2) + Safe-to-Fail Branches (Epic 3)** transforme le DAG executor en syst√®me de **speculative resilience** : ex√©cuter plusieurs hypoth√®ses simultan√©ment, conserver les succ√®s, ignorer les √©checs. Les branches sandbox √©chou√©es ne consomment que du temps CPU (ressource peu co√ªteuse), tandis que les branches MCP √©chou√©es peuvent laisser des √©tats corrompus.

**Vision architecturale** : Une gateway qui ne route pas simplement les requ√™tes, mais **orchestre intelligemment la computation** au nom des agents AI contraints par le contexte.

---

### Story Breakdown - Epic 3

**Story 3.1: Deno Sandbox Executor Foundation**

As a developer,
I want a secure Deno sandbox environment for executing agent-generated code,
So that agents can run TypeScript code without compromising system security.

**Acceptance Criteria:**
1. Sandbox module cr√©√© (`src/sandbox/executor.ts`)
2. Deno subprocess spawned avec permissions explicites (`--allow-env`, `--allow-read=~/.agentcards`)
3. Code execution isol√©e (no access to filesystem outside allowed paths)
4. Timeout enforcement (default 30s, configurable)
5. Memory limits enforcement (default 512MB heap)
6. Error capturing et structured error messages
7. Return value serialization (JSON-compatible outputs only)
8. Unit tests validating isolation (attempt to access /etc/passwd should fail)
9. Performance: Sandbox startup <100ms, code execution overhead <50ms

**Prerequisites:** Epic 2 completed (gateway operational)

---

**Story 3.2: MCP Tools Injection into Code Context**

As an agent,
I want access to MCP tools within my code execution environment,
So that I can call tools directly from my TypeScript code instead of via JSON-RPC.

**Acceptance Criteria:**
1. Tool injection system cr√©√© (`src/sandbox/context-builder.ts`)
2. MCP clients wrapped as TypeScript functions accessible in sandbox
3. Code context includes: `const github = { listCommits: async (...) => ... }`
4. Vector search used to identify relevant tools (only inject top-k, not all)
5. Type definitions generated for injected tools (TypeScript autocomplete support)
6. Tool calls from sandbox routed through existing MCP gateway
7. Error propagation: MCP errors surfaced as JavaScript exceptions
8. Integration test: Agent code calls `github.listCommits()` successfully
9. Security: No eval() or dynamic code generation in injection

**Prerequisites:** Story 3.1 (sandbox foundation)

---

**Story 3.3: Local Data Processing Pipeline**

As a user executing workflows with large datasets,
I want data to be processed locally before reaching the LLM context,
So that I save context tokens and get faster responses.

**Acceptance Criteria:**
1. Data processing pipeline implemented in sandbox
2. Agent code can: filter, map, reduce, aggregate large datasets
3. Example use case working: Fetch 1000 GitHub commits ‚Üí filter last week ‚Üí return summary
4. Context measurement: Raw data (1MB+) processed locally, summary (<1KB) returned
5. Performance benchmark: 1000-item dataset processed in <2 seconds
6. Streaming support: Large datasets streamed through processing pipeline
7. Memory efficiency: Process datasets larger than heap limit via streaming
8. Integration with DAG executor: Code execution as DAG task type
9. Metrics logged: input_size_bytes, output_size_bytes, processing_time_ms

**Prerequisites:** Story 3.2 (tools injection)

---

**Story 3.4: `agentcards:execute_code` MCP Tool**

As a Claude Code user,
I want a new MCP tool that executes my TypeScript code in AgentCards sandbox,
So that I can process data locally instead of loading everything into context.

**Acceptance Criteria:**
1. New MCP tool registered: `agentcards:execute_code`
2. Input schema: `{ code: string, intent?: string, context?: object }`
3. Intent-based mode: vector search ‚Üí inject relevant tools ‚Üí execute code
4. Explicit mode: Execute provided code with specified context
5. Output schema: `{ result: any, logs: string[], metrics: object }`
6. Error handling: Syntax errors, runtime errors, timeout errors
7. Integration with gateway: Tool appears in `list_tools` response
8. Example workflow: Claude writes code ‚Üí executes via tool ‚Üí receives result
9. Documentation: README updated with code execution examples

**Prerequisites:** Story 3.3 (data processing pipeline)

---

**Story 3.5: Safe-to-Fail Branches & Resilient Workflows**

As a developer building robust production workflows,
I want to leverage sandbox tasks as safe-to-fail branches in my DAG,
So that I can implement resilient workflows with graceful degradation and retry safety.

**Acceptance Criteria:**
1. DAG executor enhanced pour marquer sandbox tasks comme "safe-to-fail" (failure doesn't halt workflow)
2. Partial success mode: DAG continues m√™me si sandbox branches fail
3. Aggregation patterns implemented: collect results from successful branches, ignore failures
4. Example resilient workflow: Parallel analysis (fast/ML/stats) ‚Üí use first success
5. Retry logic: Failed sandbox tasks can be retried without side effects (idempotent)
6. Graceful degradation test: ML analysis timeout ‚Üí fallback to simple stats
7. A/B testing pattern: Run 2 algorithms in parallel, compare results
8. Error isolation verification: Sandbox failure doesn't corrupt MCP tasks downstream
9. Documentation: Resilient workflow patterns guide avec code examples
10. Integration test: Multi-branch workflow with intentional failures ‚Üí verify partial success

**Prerequisites:** Story 3.4 (execute_code tool)

---

**Story 3.6: PII Detection & Tokenization**

As a security-conscious user,
I want personally identifiable information (PII) automatically detected and tokenized,
So that sensitive data never reaches the LLM context.

**Acceptance Criteria:**
1. PII detection module cr√©√© (`src/sandbox/pii-detector.ts`)
2. Patterns detected: emails, phone numbers, credit cards, SSNs, API keys
3. Tokenization strategy: Replace PII with `[EMAIL_1]`, `[PHONE_1]`, etc.
4. Reverse mapping stored securely (in-memory only, never persisted)
5. Agent receives tokenized data, can reference tokens in code
6. De-tokenization happens only for final output (if needed)
7. Opt-out flag: `--no-pii-protection` for trusted environments
8. Unit tests: Validate detection accuracy (>95% for common PII types)
9. Integration test: Email in dataset ‚Üí tokenized ‚Üí agent never sees raw email

**Prerequisites:** Story 3.5 (safe-to-fail branches)

---

**Story 3.7: Code Execution Caching & Optimization**

As a developer running repetitive workflows,
I want code execution results cached intelligently,
So that I don't re-execute identical code with identical inputs.

**Acceptance Criteria:**
1. Code execution cache implemented (in-memory LRU, max 100 entries)
2. Cache key: hash(code + context + tool_versions)
3. Cache hit: Return cached result without execution (<10ms)
4. Cache invalidation: Auto-invalidate on tool schema changes
5. Cache stats logged: hit_rate, avg_latency_saved_ms
6. Configurable: `--no-cache` flag to disable caching
7. TTL support: Cache entries expire after 5 minutes
8. Persistence optional: Save cache to PGlite for cross-session reuse
9. Performance: Cache hit rate >60% for typical workflows

**Prerequisites:** Story 3.6 (PII tokenization)

---

**Story 3.8: End-to-End Code Execution Tests & Documentation**

As a developer adopting code execution,
I want comprehensive tests and documentation,
So that I understand how to use the feature effectively.

**Acceptance Criteria:**
1. E2E test suite cr√©√© (`tests/e2e/code-execution/`)
2. Test scenarios:
   - GitHub commits analysis (large dataset filtering)
   - Multi-server data aggregation (GitHub + Jira + Slack)
   - PII-sensitive workflow (email processing)
   - Error handling (timeout, syntax error, runtime error)
   - Resilient workflows with safe-to-fail branches
3. Performance regression tests added to benchmark suite
4. Documentation: README section "Code Execution Mode"
5. Examples provided: 5+ real-world use cases with code samples
6. Comparison benchmarks: Tool calls vs Code execution (context & latency)
7. Migration guide: When to use code execution vs DAG workflows
8. Security documentation: Sandbox limitations, PII protection details
9. Resilient workflow patterns comprehensive documentation
10. Video tutorial: 3-minute quickstart (optional, can be deferred)

**Prerequisites:** Story 3.7 (caching)

---

## Epic 3.5: Speculative Execution with Sandbox Isolation

**Expanded Goal (2-3 sentences):**

Impl√©menter speculation WITH sandbox pour THE feature diff√©renciateur - 0ms perceived latency avec s√©curit√© garantie. Utiliser GraphRAG community detection et confidence scoring pour pr√©dire les prochaines actions et ex√©cuter sp√©culativement dans sandbox isol√©, permettant rollback automatique si pr√©diction incorrecte. Transformer AgentCards d'un syst√®me r√©actif en syst√®me pr√©dictif qui anticipe les besoins de l'agent avant m√™me qu'il les exprime.

**Value Delivery:**

√Ä la fin de cet epic, AgentCards peut pr√©dire avec 70%+ de pr√©cision les prochaines actions d'un workflow, les ex√©cuter sp√©culativement dans sandbox isol√© pendant que l'agent r√©fl√©chit, et fournir r√©sultats instantan√©s (0ms perceived latency) quand l'agent demande finalement l'op√©ration. Les pr√©dictions incorrectes sont silencieusement discard√©es sans side effects gr√¢ce √† sandbox isolation.

**Estimation:** 1-2 stories, 3-4h

---

### Story Breakdown - Epic 3.5

**Story 3.5-1: DAG Suggester & Speculative Execution**

As an AI agent,
I want AgentCards to predict and execute likely next actions speculatively,
So that I get instant responses without waiting for execution.

**Acceptance Criteria:**
1. `DAGSuggester.predictNextNodes(current_state, context)` method implemented
2. GraphRAG community detection utilis√© pour pattern matching
3. Confidence scoring bas√© sur:
   - Historical co-occurrence frequency (85% des fois, tool A suivi de tool B)
   - Context similarity (embeddings)
   - Workflow type patterns
4. Confidence threshold: >0.70 pour speculation (configurable)
5. Speculative execution:
   - Fork workflow execution dans sandbox branch
   - Execute predicted tasks in parallel avec main workflow
   - Cache results avec confidence score
   - Discard si pr√©diction incorrecte (no side effects)
6. Integration avec AdaptiveThresholdManager (Story 4.2 - already implemented)
7. Metrics tracking:
   - Speculation hit rate (% de pr√©dictions correctes)
   - Net benefit (time saved - time wasted)
   - False positive rate (pr√©dictions incorrectes)
8. Graceful fallback: Si pr√©diction incorrecte, execute normalement
9. Tests:
   - Common pattern test: "read file" ‚Üí predict "parse json" ‚Üí verify executed speculatively
   - High confidence test: 0.85 confidence ‚Üí speculation triggered
   - Low confidence test: 0.60 confidence ‚Üí no speculation
   - Rollback test: Incorrect prediction ‚Üí discarded, no side effects
10. Performance: Speculation overhead <50ms

**Prerequisites:** Epic 3 completed (sandbox isolation), Epic 5 completed (search_tools for template discovery)

---

**Story 3.5-2: Confidence-Based Speculation & Rollback (Optional - peut √™tre merged avec 3.5-1)**

As a system administrator,
I want confidence-based speculation controls and rollback capabilities,
So that I can tune the speculation aggressiveness vs safety tradeoff.

**Acceptance Criteria:**
1. Configuration: `speculation_config.yaml`
   - `enabled: true/false`
   - `confidence_threshold: 0.70` (min confidence pour speculation)
   - `max_concurrent_speculations: 3` (resource limit)
   - `speculation_timeout: 10s` (max speculation time)
2. Rollback mechanisms:
   - Sandbox isolation ensures no side effects
   - Speculation branch discarded if incorrect
   - Main workflow unaffected by failed speculation
3. Adaptive threshold learning (integration avec Story 4.2):
   - Track speculation success/failure rates
   - Adjust threshold dynamically (too many failures ‚Üí increase threshold)
4. CLI commands:
   - `agentcards config speculation --threshold 0.75`
   - `agentcards stats speculation` ‚Üí hit rate, net benefit metrics
5. Tests:
   - Threshold tuning: Adjust threshold ‚Üí verify speculation frequency changes
   - Resource limit: Exceed max concurrent ‚Üí additional speculations queued
   - Timeout handling: Speculation exceeds 10s ‚Üí terminated, no impact on main workflow

**Prerequisites:** Story 3.5-1

---

## Epic 4: Episodic Memory & Adaptive Learning (ADR-008)

**Expanded Goal (2-3 sentences):**

√âtendre Loop 3 (Meta-Learning) avec m√©moire √©pisodique pour persistence des contextes d'ex√©cution et apprentissage adaptatif des seuils de confiance via algorithme Sliding Window + FP/FN detection. Impl√©menter storage hybride (JSONB + typed columns) permettant retrieval contextuel d'√©pisodes historiques pour am√©liorer pr√©dictions, et syst√®me d'apprentissage adaptatif ajustant dynamiquement les thresholds bas√© sur les succ√®s/√©checs observ√©s. Transformer AgentCards en syst√®me auto-am√©liorant qui apprend continuellement de ses ex√©cutions.

**Value Delivery:**

√Ä la fin de cet epic, AgentCards persiste son apprentissage entre sessions (thresholds ne sont plus perdus au red√©marrage), utilise les √©pisodes historiques pour am√©liorer pr√©dictions (context-aware), et ajuste automatiquement les thresholds de confiance pour maintenir 85%+ de success rate. Le syst√®me devient progressivement plus intelligent avec l'usage.

**Estimation:** 2 stories, 4.5-5.5h

**Note:** Story 4.2 d√©j√† impl√©ment√©e durant Epic 1 (2025-11-05). Story 4.1 **split en 2 phases** (2025-11-25) :
- **Phase 1** (Stories 4.1a/b/c) : Storage layer ind√©pendant (~2-3h) - ‚úÖ **DONE**
- **Phase 2** (Stories 4.1d/e) : Int√©grations ControlledExecutor + DAGSuggester (apr√®s Epic 2.5/3.5)

---

### Story Breakdown - Epic 4

**Story 4.1: Episodic Memory Storage & Retrieval** *(Split en Phase 1 + Phase 2)*

---

#### Phase 1 - Storage Foundation (Independent) ‚úÖ DONE 2025-11-25

**Story 4.1a: Schema PGlite** ‚úÖ

As a developer,
I want database tables for episodic events and adaptive thresholds,
So that I can persist learning data between sessions.

**Acceptance Criteria:**
1. Migration `007_episodic_memory.sql` created
2. Table `episodic_events` with hybrid schema (typed + JSONB)
3. Table `adaptive_thresholds` for threshold persistence
4. Indexes created (workflow_id, event_type, context_hash, GIN on JSONB)

**Status:** ‚úÖ Done - [Migration 007](../src/db/migrations/007_episodic_memory.sql)

---

**Story 4.1b: EpisodicMemoryStore Class** ‚úÖ

As a system developer,
I want an episodic memory store with buffered async writes,
So that event capture has <1ms overhead.

**Acceptance Criteria:**
1. `EpisodicMemoryStore` class implemented
2. Methods: `capture()`, `flush()`, `retrieveRelevant()`, `prune()`
3. Buffer system (non-blocking writes)
4. Context hash-based retrieval
5. 9 unit tests passing

**Status:** ‚úÖ Done - [episodic-memory-store.ts](../src/learning/episodic-memory-store.ts) (280 LOC, 9 tests)

---

**Story 4.1c: Threshold Persistence** ‚úÖ

As a system that learns optimal thresholds,
I want thresholds to survive server restarts,
So that learning is not lost.

**Acceptance Criteria:**
1. `AdaptiveThresholdManager` extended with PGlite client
2. `loadThresholds(context)` method added
3. `saveThresholds(context)` method added (auto-saves on adjustment)
4. Context-based threshold lookup
5. Cache layer for performance

**Status:** ‚úÖ Done - Extended [adaptive-threshold.ts](../src/mcp/adaptive-threshold.ts) (+100 LOC)

---

#### Phase 2 - Loop Integrations (After Epic 2.5/3.5) üî¥ Backlog

**Story 4.1d: ControlledExecutor Integration** üî¥

As a workflow execution system,
I want to capture episodic events automatically during execution,
So that learning happens without manual instrumentation.

**Acceptance Criteria:**
1. ControlledExecutor emits events to EpisodicMemoryStore
2. Events captured: speculation_start, task_complete, ail_decision, hil_decision
3. Context captured at each decision point
4. Integration tests verify auto-capture works

**Prerequisites:** Epic 2.5 done (ControlledExecutor exists)

---

**Story 4.1e: DAGSuggester Context Boost** üî¥

As an AI agent,
I want DAGSuggester to use past episodes for better predictions,
So that recommendations improve based on historical success.

**Acceptance Criteria:**
1. DAGSuggester queries similar episodes before suggesting
2. Confidence boost if similar episode succeeded
3. Avoid patterns that failed historically
4. Integration tests verify context-aware suggestions

**Prerequisites:** Epic 3.5 done (DAGSuggester with speculation)

---

**Story 4.2: Adaptive Threshold Learning (Sliding Window + FP/FN Detection)**

**Status:** ‚úÖ COMPLETED (Implemented 2025-11-05 during Epic 1, documented 2025-11-24)

**Implementation:** See `/home/ubuntu/CascadeProjects/AgentCards/docs/stories/story-4.2.md`

As an AI agent,
I want the system to learn optimal confidence thresholds from execution feedback,
So that I can reduce unnecessary manual confirmations while avoiding failed speculative executions.

**Acceptance Criteria:** (All criteria met - see story file for details)
1. ‚úÖ Sliding window algorithm tracks last 50 executions
2. ‚úÖ Analyzes 20 most recent executions every 10 executions
3. ‚úÖ Increases threshold when False Positive Rate > 20% (failed speculation)
4. ‚úÖ Decreases threshold when False Negative Rate > 30% (unnecessary confirmations)
5. ‚úÖ Respects min (0.40) and max (0.90) threshold bounds
6. ‚úÖ Provides metrics for monitoring (success rate, wasted compute, saved latency)
7. ‚úÖ Integration with GatewayHandler for real-time threshold adaptation
8. ‚úÖ 8 passing unit tests covering all adaptive behaviors

**Implementation Details:**
- File: `src/mcp/adaptive-threshold.ts` (195 LOC)
- Algorithm: Sliding Window (50 executions) + False Positive/Negative detection
- Thresholds persist in memory beyond sliding window (not lost after 50 executions)
- **No disk persistence yet** - Story 4.1 will add PGlite storage for session continuity
- **Complementary to ADR-015 (Story 5.1):**
  - Story 4.2: Adapts **thresholds** based on success/failure rates
  - Story 5.1: Improves **search quality** via graph-based boost
  - Both reduce "too many manual confirmations" via different mechanisms

**Prerequisites:** Story 4.1 (episodic memory provides disk persistence)

---

## Epic 5: Intelligent Tool Discovery & Graph-Based Recommendations

### Vision

Am√©liorer la d√©couverte d'outils en combinant recherche s√©mantique et recommandations bas√©es sur les patterns d'usage r√©els. Le probl√®me initial: `execute_workflow` utilisait PageRank pour la recherche d'un seul outil, ce qui n'a pas de sens (PageRank mesure l'importance globale, pas la pertinence √† une requ√™te).

### Technical Approach

**Hybrid Search Pipeline (style Netflix):**
1. **Candidate Generation** - Recherche s√©mantique (vector embeddings)
2. **Re-ranking** - Graph-based boost (Adamic-Adar, neighbors)
3. **Final Filtering** - Top-K results

**Algorithmes Graphology:**
- `Adamic-Adar` - Similarit√© bas√©e sur voisins communs rares
- `getNeighbors(in/out/both)` - Outils souvent utilis√©s avant/apr√®s
- `computeGraphRelatedness()` - Score hybride avec contexte

**Alpha Adaptatif:**
- `Œ± = 1.0` si 0 edges (pure semantic)
- `Œ± = 0.8` si < 10 edges
- `Œ± = 0.6` si > 50 edges (balanced)

### Story Breakdown - Epic 5

---

**Story 5.1: search_tools - Semantic + Graph Hybrid Search**

As an AI agent,
I want a dedicated tool search endpoint that combines semantic and graph-based recommendations,
So that I can find relevant tools without threshold failures.

**Acceptance Criteria:**
1. Nouvel outil MCP `agentcards:search_tools` expos√© dans tools/list
2. Recherche s√©mantique pure (pas de seuil bloquant comme execute_workflow)
3. Alpha adaptatif: plus de poids s√©mantique quand graphe sparse
4. Support `context_tools` pour booster les outils li√©s au contexte actuel
5. Option `include_related` pour obtenir les voisins du graphe
6. GraphRAGEngine: `getNeighbors()`, `computeAdamicAdar()`, `adamicAdarBetween()`
7. GraphRAGEngine: `computeGraphRelatedness()` avec normalisation
8. Logs d√©taill√©s: query, alpha, edge_count, scores
9. Tests HTTP: queries "screenshot", "list files", "search web"

**Prerequisites:** Epic 3 (sandbox, execute_code)

---

**Story 5.2: Workflow Templates & Graph Bootstrap**

As a system administrator,
I want predefined workflow templates to initialize the graph,
So that recommendations work even before real usage data is collected.

**Acceptance Criteria:**
1. Fichier `config/workflow-templates.yaml` avec templates courants
2. Templates: web_research, browser_automation, file_operations, knowledge_management
3. GraphRAGEngine: `bootstrapFromTemplates()` method
4. Chargement automatique au d√©marrage si graph vide (0 edges)
5. Edges marqu√©es `source: 'template'` pour distinguer du real usage
6. PageRank et m√©triques recalcul√©es apr√®s bootstrap
7. Tests: v√©rifier que bootstrap cr√©e les edges attendues
8. Documentation: comment ajouter de nouveaux templates

**Prerequisites:** Story 5.1

---

## Epic 6: Real-time Graph Monitoring & Observability

### Vision

Fournir une visibilit√© compl√®te sur l'√©tat du graphe de d√©pendances en temps r√©el via un dashboard interactif. Les d√©veloppeurs et power users pourront observer comment le graphe apprend et √©volue, diagnostiquer les probl√®mes de recommandations, et comprendre quels outils sont r√©ellement utilis√©s ensemble dans leurs workflows.

**Probl√®me:** Actuellement, le graphe est une "bo√Æte noire" - les edges, PageRank, et communities sont invisibles. Impossible de d√©bugger pourquoi une recommandation est faite, ou de visualiser l'√©volution du graphe au fil du temps.

### Value Delivery

√Ä la fin de cet epic, un d√©veloppeur peut ouvrir le dashboard AgentCards et voir en direct :
- Le graphe complet avec nodes (tools) et edges (d√©pendances)
- Les √©v√©nements en temps r√©el (edge cr√©√©, workflow ex√©cut√©)
- Les m√©triques live (edge count, density, alpha adaptatif)
- Les outils les plus utilis√©s (PageRank top 10)
- Les communities d√©tect√©es par Louvain
- Les chemins de d√©pendances entre outils

**Estimation:** 4 stories, ~8-12h

### Technical Approach

**Architecture:**
- **Backend**: SSE endpoint `/events/stream` pour √©v√©nements temps r√©el
- **Frontend**: Page HTML statique avec D3.js/Cytoscape.js pour graph viz
- **Data Flow**: GraphRAGEngine ‚Üí EventEmitter ‚Üí SSE ‚Üí Browser
- **Performance**: Graph rendering <500ms pour 200 nodes

**Event Types:**
- `graph_synced` - Graph recharg√© depuis DB
- `edge_created` - Nouvelle d√©pendance d√©tect√©e
- `edge_updated` - Confidence score augment√©
- `workflow_executed` - DAG ex√©cut√© avec succ√®s
- `metrics_updated` - PageRank/communities recalcul√©s

---

### Story Breakdown - Epic 6

---

**Story 6.1: Real-time Events Stream (SSE)**

As a developer monitoring AgentCards,
I want to receive graph events in real-time via Server-Sent Events,
So that I can observe how the system learns without polling.

**Acceptance Criteria:**
1. SSE endpoint cr√©√©: `GET /events/stream`
2. EventEmitter int√©gr√© dans GraphRAGEngine
3. Event types: `graph_synced`, `edge_created`, `edge_updated`, `workflow_executed`, `metrics_updated`
4. Event payload: timestamp, event_type, data (tool_ids, scores, etc.)
5. Reconnection automatique si connexion perdue (client-side retry logic)
6. Heartbeat events toutes les 30s pour maintenir la connexion
7. Max 100 clients simultan√©s (√©viter DoS)
8. CORS headers configur√©s pour permettre frontend local
9. Tests: curl stream endpoint, v√©rifier format events
10. Documentation: Event schema et exemples

**Prerequisites:** Epic 5 completed (search_tools functional)

---

**Story 6.2: Interactive Graph Visualization Dashboard**

As a power user,
I want a web interface to visualize the tool dependency graph,
So that I can understand which tools are used together.

**Acceptance Criteria:**
1. Page HTML statique: `public/dashboard.html`
2. Force-directed graph layout avec D3.js ou Cytoscape.js
3. Nodes = tools (couleur par server, taille par PageRank)
4. Edges = d√©pendances (√©paisseur par confidence_score)
5. Interactions: zoom, pan, drag nodes
6. Click sur node ‚Üí affiche details (name, server, PageRank, neighbors)
7. Real-time updates via SSE (nouveaux edges anim√©s)
8. L√©gende interactive (filtres par server)
9. Performance: render <500ms pour 200 nodes
10. Endpoint static: `GET /dashboard` sert le HTML
11. Mobile responsive (optionnel mais nice-to-have)

**Prerequisites:** Story 6.1 (SSE events)

---

**Story 6.3: Live Metrics & Analytics Panel**

As a developer,
I want to see live metrics about graph health and recommendations,
So that I can monitor system performance and debug issues.

**Acceptance Criteria:**
1. Metrics panel dans dashboard (sidebar ou overlay)
2. Live metrics affich√©s:
   - Edge count, node count, density
   - Alpha adaptatif actuel
   - PageRank top 10 tools
   - Communities count (Louvain)
   - Workflow success rate (derni√®res 24h)
3. Graphiques time-series (Chart.js/Recharts):
   - Edge count over time
   - Average confidence score over time
   - Workflow execution rate (workflows/hour)
4. API endpoint: `GET /api/metrics` retourne JSON
5. Auto-refresh toutes les 5s (ou via SSE)
6. Export metrics: bouton "Download CSV"
7. Date range selector: last 1h, 24h, 7d
8. Tests: v√©rifier que metrics endpoint retourne donn√©es correctes

**Prerequisites:** Story 6.2 (dashboard)

---

**Story 6.4: Graph Explorer & Search Interface**

As a user,
I want to search and explore the graph interactively,
So that I can find specific tools and understand their relationships.

**Acceptance Criteria:**
1. Search bar dans dashboard: recherche par tool name/description
2. Autocomplete suggestions pendant typing
3. Click sur r√©sultat ‚Üí highlight node dans graph
4. "Find path" feature: s√©lectionner 2 nodes ‚Üí affiche shortest path
5. Filtres interactifs:
   - Par server (checkboxes)
   - Par confidence score (slider: 0-1)
   - Par date (edges created after X)
6. Adamic-Adar visualization: hover sur node ‚Üí affiche related tools avec scores
7. Export graph data: bouton "Export JSON/GraphML"
8. Breadcrumb navigation: retour √† vue compl√®te apr√®s zoom
9. Keyboard shortcuts: `/` pour focus search, `Esc` pour clear selection
10. API endpoint: `GET /api/tools/search?q=screenshot` pour autocomplete

**Prerequisites:** Story 6.3 (metrics panel)

---

## Story Guidelines Reference

**Story Format:**

```
**Story [EPIC.N]: [Story Title]**

As a [user type],
I want [goal/desire],
So that [benefit/value].

**Acceptance Criteria:**
1. [Specific testable criterion]
2. [Another specific criterion]
3. [etc.]

**Prerequisites:** [Dependencies on previous stories, if any]
```

**Story Requirements:**

- **Vertical slices** - Complete, testable functionality delivery
- **Sequential ordering** - Logical progression within epic
- **No forward dependencies** - Only depend on previous work
- **AI-agent sized** - Completable in 2-4 hour focused session
- **Value-focused** - Integrate technical enablers into value-delivering stories

---

**For implementation:** Use the `create-story` workflow to generate individual story implementation plans from this epic breakdown.
