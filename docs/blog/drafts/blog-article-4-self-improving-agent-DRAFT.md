# Adaptive Learning: How AgentCards Learns the Optimal Strategy for Every Workflow

**Auteur:** AgentCards Team
**Date:** Janvier 2025
**Statut:** ğŸš§ DRAFT - Plan dÃ©taillÃ© Ã  complÃ©ter
**Sujets:** Machine Learning, Adaptive Systems, Meta-Learning, Self-Improvement

---

## PLAN ARTICLE 3: ADAPTIVE LEARNING

**Objectif:** Expliquer comment AgentCards apprend et s'amÃ©liore au fil du temps via l'architecture 3-Loop Learning, la mÃ©moire Ã©pisodique, et les seuils adaptatifs.

**Epic concernÃ©:** Epic 2.5 - Adaptive DAG Feedback Loops (ADR-008)

**Longueur cible:** ~5,500 mots

**Tone:** Technique mais accessible, focus sur le "pourquoi" avant le "comment"

---

## Introduction (500 mots)

### Accroche

> "Dans les Articles 1 et 2, nous avons explorÃ© comment AgentCards optimise l'exÃ©cution via semantic gateways, DAG parallÃ¨le, et exÃ©cution spÃ©culative. Mais il y avait un dÃ©tail cachÃ© : **tous les seuils Ã©taient fixes**. Le seuil de confiance pour la spÃ©culation ? 0.7. Le nombre d'outils Ã  exposer ? 20. Ces valeurs fonctionnent... mais sont-elles optimales ? Et si le systÃ¨me pouvait les apprendre ?"

### Le problÃ¨me des paramÃ¨tres statiques

**[Ã€ COMPLÃ‰TER]**
- Speculation threshold fixe (0.7) â†’ trop agressive pour certains workflows, trop conservative pour d'autres
- Exemple concret : data_analysis workflows (prÃ©cision critique) vs web_scraping (vitesse prioritaire)
- Le coÃ»t d'une mauvaise calibration : computation gaspillÃ©e OU opportunitÃ©s manquÃ©es

### Ce que vous allez apprendre

Dans cet article, nous explorons :
1. **Architecture 3-Loop Learning** â€” Comment le systÃ¨me apprend Ã  3 niveaux (Execution, Adaptation, Meta-Learning)
2. **Episodic Memory** â€” Stocker les expÃ©riences passÃ©es pour amÃ©liorer les prÃ©dictions futures
3. **Adaptive Thresholds** â€” Apprendre le seuil de confiance optimal pour chaque type de workflow
4. **Convergence Algorithm** â€” Comment le systÃ¨me trouve et maintient l'Ã©quilibre optimal

---

## Section 1: Le problÃ¨me des seuils fixes (800 mots)

### 1.1 Rappel: La spÃ©culation avec threshold fixe (Article 2)

**[Ã€ COMPLÃ‰TER]**
- Recap rapide de l'exÃ©cution spÃ©culative (Article 2, Concept 4)
- GraphRAG prÃ©dit le DAG avec score de confiance
- Si confiance > 0.7 â†’ spÃ©culer
- ProblÃ¨me : **le 0.7 est arbitraire**

### 1.2 Le dilemme prÃ©cision vs vitesse

**[Ã€ COMPLÃ‰TER - Tableau comparatif]**

```
Threshold trop bas (0.5):
  âœ… Specule souvent â†’ latence basse
  âŒ Taux d'Ã©chec Ã©levÃ© â†’ computation gaspillÃ©e

Threshold trop haut (0.9):
  âœ… Taux de succÃ¨s Ã©levÃ© â†’ peu de gaspillage
  âŒ Specule rarement â†’ opportunitÃ©s manquÃ©es

Threshold optimal (variable selon workflow):
  âœ… Balance prÃ©cision et vitesse
  âœ… S'adapte au contexte
  âœ… Ã‰volue avec l'expÃ©rience
```

### 1.3 Pourquoi un threshold unique ne suffit pas

**[Ã€ COMPLÃ‰TER]**
- DiffÃ©rents types de workflows ont des patterns diffÃ©rents
- data_analysis : PrÃ©dictions difficiles, besoin de 0.85+ threshold
- file_operations : PrÃ©dictions faciles, 0.65 threshold suffit
- Citation benchmark : Success rate varie de 62% Ã  94% selon workflow type

### 1.4 La solution: Apprentissage adaptatif

**[Ã€ COMPLÃ‰TER - Transition]**
- Introduction de l'idÃ©e: "Et si chaque workflow type apprenait son propre threshold optimal ?"
- Analogie: Machine learning pour hyperparameter tuning, mais pour l'orchestration d'agents
- Lead-in vers l'architecture 3-Loop

---

## Section 2: Architecture 3-Loop Learning (1200 mots)

### 2.1 Vue d'ensemble: Trois niveaux d'apprentissage

**[Ã€ COMPLÃ‰TER - Diagramme ASCII]**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ARCHITECTURE 3-LOOP LEARNING                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Loop 1: EXECUTION (Real-time, milliseconds)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ Event stream observable                             â”‚ â”‚
â”‚  â”‚ â€¢ Task execution with results                         â”‚ â”‚
â”‚  â”‚ â€¢ Command queue for control                           â”‚ â”‚
â”‚  â”‚ â†’ Capture: What happened?                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                          â†“ Events                           â”‚
â”‚                                                             â”‚
â”‚  Loop 2: ADAPTATION (Runtime, seconds)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ Agent-in-the-Loop (AIL) decisions                   â”‚ â”‚
â”‚  â”‚ â€¢ Human-in-the-Loop (HIL) approval                    â”‚ â”‚
â”‚  â”‚ â€¢ Dynamic DAG replanning                              â”‚ â”‚
â”‚  â”‚ â†’ Act: What should we do differently?                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                          â†“ Decisions                        â”‚
â”‚                                                             â”‚
â”‚  Loop 3: META-LEARNING (Per-workflow, minutes-days)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ Episodic memory storage                             â”‚ â”‚
â”‚  â”‚ â€¢ Adaptive threshold learning                         â”‚ â”‚
â”‚  â”‚ â€¢ GraphRAG updates                                    â”‚ â”‚
â”‚  â”‚ â†’ Learn: How can we improve for next time?           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Loop 1: Execution - Capture Everything

**[Ã€ COMPLÃ‰TER]**
- Event stream avec types d'Ã©vÃ©nements : speculation_start, task_complete, ail_decision, hil_decision
- Capture non-blocking (<1ms overhead)
- Exemple d'event:
  ```typescript
  {
    id: "evt_123",
    type: "speculation_start",
    workflow_id: "wf_789",
    task_id: "t1",
    data: {
      prediction: { toolId: "json:parse", confidence: 0.82 },
      context: { workflowType: "data_analysis" }
    }
  }
  ```

### 2.3 Loop 2: Adaptation - React Intelligently

**[Ã€ COMPLÃ‰TER]**
- DÃ©cisions pendant l'exÃ©cution (AIL/HIL) â†’ dÃ©tails dans Article 4
- Teaser seulement ici : "Loop 2 permet au systÃ¨me de s'adapter en temps rÃ©el. Nous explorerons cela en profondeur dans l'Article 4."
- Focus: Comment les dÃ©cisions sont capturÃ©es pour Loop 3

### 2.4 Loop 3: Meta-Learning - Improve Over Time

**[Ã€ COMPLÃ‰TER]**
- Base Loop 3 (Article 2): GraphRAG updates (co-occurrence patterns)
- Extended Loop 3 (cet article): Episodic Memory + Adaptive Thresholds
- Key insight: "Loop 3 ne modifie PAS le code (comme CoALA suggÃ¨re), il modifie les **paramÃ¨tres de configuration** (thresholds, weights)"
- SÃ©curitÃ©: Bornes (0.70-0.95), isolation par workflow

### 2.5 Comparaison avec CoALA Framework

**[Ã€ COMPLÃ‰TER]**
- CoALA: 2 loops (Decision Cycle, Learning Loop)
- CoALA identifie "meta-learning via code modification" comme thÃ©orique mais risquÃ©
- AgentCards: 3 loops avec Loop 2 unique (adaptation runtime)
- AgentCards implÃ©mente meta-learning via configuration learning (sÃ»r et pratique)

---

## Section 3: Episodic Memory - The Learning Bridge (1500 mots)

### 3.1 Qu'est-ce que la mÃ©moire Ã©pisodique ?

**[Ã€ COMPLÃ‰TER]**
- DÃ©finition: Stockage d'Ã©pisodes (sÃ©quences d'Ã©vÃ©nements) d'exÃ©cutions passÃ©es
- Inspiration: MÃ©moire Ã©pisodique humaine (rappel d'expÃ©riences spÃ©cifiques)
- Vs mÃ©moire sÃ©mantique (GraphRAG = connaissances gÃ©nÃ©rales)

### 3.2 Architecture de stockage: Hybrid JSONB + Typed Columns

**[Ã€ COMPLÃ‰TER]**
- Schema PGlite:
  ```sql
  CREATE TABLE episodic_events (
    id TEXT PRIMARY KEY,
    workflow_id TEXT NOT NULL,
    event_type TEXT NOT NULL, -- Typed
    task_id TEXT,              -- Typed
    timestamp TIMESTAMPTZ,
    data JSONB NOT NULL        -- Flexible
  );
  ```
- Rationale: Type safety pour queries communes, flexibilitÃ© pour event-specific data
- Indexes: workflow_id, event_type, timestamp, GIN sur JSONB

### 3.3 Que stocke-t-on ?

**[Ã€ COMPLÃ‰TER - Exemples de chaque type d'event]**

1. **speculation_start**: PrÃ©diction faite, pas encore confirmÃ©e
2. **speculation_resolved**: PrÃ©diction confirmÃ©e (correct/incorrect)
3. **task_complete**: TÃ¢che terminÃ©e avec rÃ©sultat (success/error)
4. **ail_decision**: Agent a pris une dÃ©cision
5. **hil_decision**: Humain a approuvÃ©/rejetÃ©

### 3.4 Retrieval: Context-Aware Query

**[Ã€ COMPLÃ‰TER]**
- Hash de contexte: `workflowType:data_analysis|domain:finance|complexity:medium`
- Query: Trouver Ã©pisodes similaires (mÃªme hash)
- Exemple:
  ```typescript
  const relevantEpisodes = await episodicMemory.retrieveRelevant({
    workflowType: 'data_analysis',
    domain: 'finance'
  }, { limit: 50 });
  ```

### 3.5 Boost de confiance via expÃ©rience passÃ©e

**[Ã€ COMPLÃ‰TER]**
- Scenario: GraphRAG prÃ©dit `xml:parse` avec confidence 0.68
- Query episodic memory: "xml:parse a-t-il rÃ©ussi dans workflows similaires ?"
- Trouve 5 episodes oÃ¹ xml:parse a rÃ©ussi â†’ boost +0.05
- Nouvelle confidence: 0.73 â†’ au-dessus du threshold â†’ spÃ©cule!
- Benchmark: Boost typique de +2% Ã  +10% selon historique

### 3.6 Retention Policy & Performance

**[Ã€ COMPLÃ‰TER]**
- Hybrid retention: 30 jours OU 10,000 events (whatever comes first)
- Batch writes: Buffer 50 events â†’ flush async
- Performance: <1ms capture, <10ms retrieval, <5MB storage pour 10K events

---

## Section 4: Adaptive Thresholds - Finding the Sweet Spot (1800 mots)

### 4.1 Le problÃ¨me de calibration

**[Ã€ COMPLÃ‰TER]**
- Threshold trop bas â†’ Waste computation
- Threshold trop haut â†’ Miss opportunities
- Goal: Target success rate 80-90% (sweet spot empirique)

### 4.2 StratÃ©gie Conservative-Start

**[Ã€ COMPLÃ‰TER]**
- Pourquoi commencer Ã  0.92 (conservateur) ?
  - Minimiser waste en phase d'apprentissage
  - Ã‰viter oscillation
  - Garantir high success rate initial
- Cold start acceptable: Premiers workflows plus lents, c'est OK

### 4.3 L'algorithme EMA (Exponential Moving Average)

**[Ã€ COMPLÃ‰TER - Code commentÃ©]**

```typescript
class AdaptiveThresholdManager {
  private config = {
    initial: 0.92,           // Conservative start
    min: 0.70,               // Don't go below (too risky)
    max: 0.95,               // Don't go above (misses opportunities)
    targetSuccessRate: 0.85, // Sweet spot
    learningRate: 0.05,      // Smoothing factor (5%)
    evaluationWindow: 50     // Samples before adjustment
  };

  adjustThreshold(current: number, successRate: number): number {
    // [Ã€ COMPLÃ‰TER - Algorithme dÃ©taillÃ©]
    // 1. Calculer optimal threshold basÃ© sur success rate
    // 2. Appliquer EMA smoothing
    // 3. Clamp aux bounds
  }
}
```

### 4.4 Convergence Simulation - Week by Week

**[Ã€ COMPLÃ‰TER - Timeline narrative]**

```
Week 1: Cold Start
  Context: data_analysis workflows
  Threshold: 0.92 (initial)
  Speculations: 50
  Success rate: 92%
  Analysis: Too conservative (success rate > 90%)
  Adjustment: Lower to 0.88 (-0.04)

Week 2: Learning Phase
  Threshold: 0.88
  Speculations: 80
  Success rate: 87%
  Analysis: Near target (85%), slight adjustment
  Adjustment: Lower to 0.86 (-0.02)

Week 3: Convergence
  Threshold: 0.86
  Speculations: 100
  Success rate: 85%
  Analysis: âœ… Target achieved!
  Adjustment: Hold at 0.86 (converged)

Week 4+: Stable Operation
  Threshold: 0.86 (stable)
  Success rate: 83-87% (variance within acceptable range)
  System: Auto-optimized for data_analysis workflows
```

### 4.5 Per-Workflow-Type Learning

**[Ã€ COMPLÃ‰TER - Tableau comparatif]**

```
Workflow Type      | Converged Threshold | Success Rate | Speculations/Week
-------------------|--------------------|--------------|-----------------
data_analysis      | 0.86               | 85%          | 120
web_scraping       | 0.73               | 88%          | 200
file_operations    | 0.68               | 91%          | 350
api_integration    | 0.82               | 83%          | 150
```

Insight: DiffÃ©rents workflows â†’ diffÃ©rents thresholds optimaux

### 4.6 DÃ©tection de convergence

**[Ã€ COMPLÃ‰TER]**
- CritÃ¨re 1: Success rate dans target range (80-90%)
- CritÃ¨re 2: Variance < 0.02 sur 5 derniÃ¨res Ã©valuations
- CritÃ¨re 3: Minimum 50 samples
- Action: Marquer comme "converged", monitoring seulement

### 4.7 Handling Drift - Quand le contexte change

**[Ã€ COMPLÃ‰TER]**
- Scenario: Nouveau tool ajoutÃ© au workflow type
- Impact: Success rate drop de 85% Ã  72%
- DÃ©tection: Success rate < 80% pendant 3 Ã©valuations
- Action: Re-learn (reset convergence flag, reprendre ajustements)

---

## Section 5: The Symbiotic Learning Loop (800 mots)

### 5.1 Comment tout travaille ensemble

**[Ã€ COMPLÃ‰TER - Flow diagram]**

```
1. Execution (Loop 1):
   â†’ Capture speculation outcomes dans episodic memory

2. After 50 speculations:
   â†’ Query episodic memory pour ce workflow type
   â†’ Calculate success rate

3. Meta-Learning (Loop 3):
   â†’ Adjust threshold via EMA algorithm
   â†’ Store new threshold in adaptive_thresholds table

4. Next workflow (mÃªme type):
   â†’ Use new threshold
   â†’ Speculation plus optimisÃ©e
   â†’ Capture outcomes â†’ feedback loop continues
```

### 5.2 Episodic Memory boosts Adaptive Thresholds

**[Ã€ COMPLÃ‰TER]**
- Episodic memory fournit les donnÃ©es pour learning
- Sans episodic memory â†’ pas de success rate tracking
- Symbiosis: Episodic stocke, Adaptive apprend

### 5.3 Adaptive Thresholds creates better Episodes

**[Ã€ COMPLÃ‰TER]**
- Meilleur threshold â†’ meilleures prÃ©dictions
- Meilleures prÃ©dictions â†’ plus d'Ã©pisodes rÃ©ussis
- Plus d'Ã©pisodes rÃ©ussis â†’ meilleur historical context
- Cycle vertueux

---

## Section 6: Production Implications (600 mots)

### 6.1 Cold Start Strategy

**[Ã€ COMPLÃ‰TER]**
- Premiers workflows: Conservative (0.92)
- Acceptable: LÃ©gÃ¨rement plus lents
- Mitigation: Pre-seed avec thresholds par dÃ©faut si domaine connu

### 6.2 Privacy & Security

**[Ã€ COMPLÃ‰TER]**
- Episodic events: Pas de PII, seulement metadata
- Context hashing: Pas de donnÃ©es sensibles
- Thresholds: Configuration, pas de code

### 6.3 Observability

**[Ã€ COMPLÃ‰TER]**
- Metrics dashboard: Threshold evolution par workflow type
- Success rate graphs
- Convergence status indicators

### 6.4 Rollback & Control

**[Ã€ COMPLÃ‰TER]**
- Manual override: Forcer un threshold si needed
- Reset: Revenir Ã  initial 0.92 si drift dÃ©tectÃ©
- Disable learning: Option pour production critique

---

## Section 7: CoALA vs AgentCards - Meta-Learning Comparison (500 mots)

### 7.1 CoALA's Meta-Learning Vision

**[Ã€ COMPLÃ‰TER]**
- CoALA propose: "Modify agent code" (procedural memory)
- Status: Theoretical, risky, not implemented
- Concerns: Safety, alignment, bugs

### 7.2 AgentCards' Pragmatic Approach

**[Ã€ COMPLÃ‰TER]**
- Apprendre les **paramÃ¨tres**, pas le code
- Safe: Bounded (0.70-0.95), isolated per workflow
- Practical: Implemented, tested, converges in 2-3 weeks

### 7.3 Future: Toward Code-Level Learning?

**[Ã€ COMPLÃ‰TER]**
- Peut-Ãªtre: Learn DAG structure patterns (not just thresholds)
- Maybe: Learn tool selection preferences
- Careful: Safety boundaries always

---

## Conclusion (500 mots)

### Recap: From Static to Adaptive

**[Ã€ COMPLÃ‰TER]**
- Articles 1-2: Static optimizations (gateway, DAG, speculation)
- Article 3: Adaptive optimizations (learning, improving)
- Key difference: System gets **better over time**

### The Vision: Self-Improving Agents

**[Ã€ COMPLÃ‰TER]**
- Imagine: Agent qui optimise automatiquement pour chaque use case
- No manual tuning: System finds optimal parameters
- Continuous improvement: Chaque execution amÃ©liore le prochain

### What's Next

**[Ã€ COMPLÃ‰TER]**
- Article 4: Human-in-the-Loop & Dynamic Adaptation (Loop 2 details)
- Production: AgentCards implementing adaptive learning now
- Community: Open questions sur meta-learning safety

---

## Annexes

### A. Benchmarks DÃ©taillÃ©s

**[Ã€ COMPLÃ‰TER - Tableaux de donnÃ©es]**
- Convergence time par workflow type
- Success rate evolution
- Storage overhead

### B. Formules MathÃ©matiques

**[Ã€ COMPLÃ‰TER]**
- EMA formula dÃ©taillÃ©e
- Success rate calculation
- Convergence detection algorithm

### C. Pseudocode Complet

**[Ã€ COMPLÃ‰TER]**
- AdaptiveThresholdManager class
- EpisodicMemoryStore class
- Integration dans SpeculativeExecutor

---

**Ã€ propos d'AgentCards** : AgentCards est une exploration open-source de patterns architecturaux avancÃ©s pour les agents MCP. Le code complet et les benchmarks sont disponibles sur GitHub.

**Questions ou feedback ?** Nous serions ravis d'entendre vos retours sur l'adaptive learning. Ces patterns devraient-ils Ãªtre standardisÃ©s dans l'Ã©cosystÃ¨me MCP ? Contactez-nous sur notre dÃ©pÃ´t GitHub.

---

**STATUT DRAFT:**
- âœ… Structure complÃ¨te dÃ©finie
- â³ Sections Ã  remplir (marquÃ©es [Ã€ COMPLÃ‰TER])
- â³ Diagrammes Ã  crÃ©er
- â³ Benchmarks Ã  ajouter
- â³ Review technique nÃ©cessaire

**PROCHAINES Ã‰TAPES:**
1. ComplÃ©ter sections une par une
2. CrÃ©er diagrammes ASCII
3. Ajouter benchmarks rÃ©els
4. Review avec Ã©quipe technique
5. Publication aprÃ¨s Article 2
